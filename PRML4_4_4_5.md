### 4.4 Laplace Approximation
#### goals
- posterior distribution is in a logistic regression form, therefore, new methods need to be introduced
- **Laplace approximation** aims to find a Gaussian approximation to a probability density defined over a set of continuous variables
#### how
- find a Gaussian approximation which is centered on a **mode** of the original distribution
    - **mode** : a point $z_0$ such that $p'(z_0)=0$ and $p(z)=\frac{1}{Z}f(z)$ where $Z$ is the normalization parameter
- the logarithm of a Gaussian distribution is a quadratic function, we therefore consider a Taylor expansion of $f(z)$ centered on the mode $z_0$. Some terms of the Taylor expansion are zero due to the mode. After some simple transformations, the answer is found. (P214)
- In M-dimensional space, instead of a mode, a Hessian matrix is needed.
### applicable situation and disadvantages
- *best-fit situation* : According to central limit theorem, the posterior distribution for a model is expected to become increasingly better approximated by a Gaussian as the number of observed points increased.
- *disadvantages* : only directly applicable to real variables and is based purely on the aspects of the true distribution at a specific value of the variable, and so can fail to capture important global properties.

### 4.4.1 Model comparison and BIC
- **BICï¼ˆBayesian Information Criterion)** : using the method of Laplace approximation to approximate $p(D) = \int p(D|\theta)p(\theta)d\theta$, assume the Hessian matrix has full rank, then $lnp(D)$ could be roughly estimated.

### 4.5 Bayesian Logistic Regression
### Background info
- Exact Bayesian inference for logistic regression is intractable. Therefore, consider the application of Laplace approximation.
- **Logistic regression :** $p(C_1|\phi)=y(\phi)=\sigma(w^T\phi)$ for a data set $\{\phi_n, t_n\}$ where $t_n$ are the class labels
#### Posterior for w
- begin with a Gaussian prior $p(w)=N(w|m_0, s_0)$, then the posterior distribution over $w$ is :
$$ p(w|t) \propto p(w)p(t|w)$$
where the likelihood function $p(t|w)=\prod_{n=1}^Ny_n^{t_n}\{{1-y_n}^{1-t_n}\}$
- taking the log both sides, maximize the logarithm of the posterior distribution(**MAP**) which will define the mean of the posterior distribution, the covariance will be given by the inverse of the matrix of second derivatives of the negative logarithm, then we get a Gaussian approximation of the posteior distribution :
$$ q(w)=N(w_{MAP}, S_N)$$
#### Predictive distribution
