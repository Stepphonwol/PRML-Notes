## 1.4
- **the curse of dimensionality** : not all intuitions developed in spaces of low dimensionalty will generalize to spaces of many dimensions
- **finding effective techniques applicable to higher dimensions** : 
    - core data may be confined in smaller dimensions, the probability that the data is distributed in all the dimensions is approaching to zero
    - local interpolation-like techniques to make predictions due to the smoothness(at least locally) properties of real data

## 1.5 Decision Theory
### situation : a medical diagnosis problem(classification)
- the subject of **decision theory** : make optimal decisions giving the proper probablisitic description(p(x, t)).
- the determination of the joint probability distribution of p(x, t) is called **inference**
#### 1.5.1 Minimizing the misclassification rate
##### terms
- **decision regions**: input space is divided into several parts whichever is assigned a tag(one for each class). 
$$All\quad points\quad in \quad R_k\quad are\quad assigned\quad to\quad class\quad C_k$$
- **decision boundaries(surfaces)**: boundaries between decision regions
- *each decision region doesn't need to be contiguous but could comprise several disjoint regions*
##### optimal decision rule
1. **the minimum probability of making a mistake is obtained if each value of x is assigned to the class for which the posterior probability is largest**
2. In other words, to minimize(two classes):
$$ p(mistake)=\int_{R_1}p(x, C_2)dx+\int_{R_2}p(x, C_1)dx$$
3. Based on the schematic illustration of the joint probability distribution, the optimal decision is then equivalent to minimize the size of certain area.
4. For k classes, to maximize:
$$ p(correct) = \sum_{k=1}^K\int_{R_k}p(\bold{x}, C_k)dx$$
#### 1.5.2 Minimizing the expected loss
1. The cost of different types of misclassification is different.
2. In order to minimize the overall loss in misclassification, **loss function** and **loss matrix** are used.
    - the (k, j) entry of the loss matrix $L$ means the loss of classifying x who really belongs to class $C_k$ to class $C_j$
    - loss function(average loss):
    $$ E[L]=\sum_k\sum_j\int_{R_j}L_{kj}p(\bold{X}, C_k)dx$$
3. Minimize the average loss function above which is trivial to do once we figure out the joint probablity distribution.
#### 1.5.3 The reject option
- Introducing a threhold θ, for those inputs x whose largest posterior probablity $p(C_k|x)$ is less than or equal to θ, we reject them(omit them).
#### 1.5.4 Inference and decision
- A general classification problem could be divided into two stages.
    - *inference* stage : to determine the posterior probability distribution 
    - *decision* stage : make optimal options based on the probability distribution
- Three different methods are able to solve the classification problem.
    - **generative models** : determining $p(x|C_k)$ or $p(x, C_k)$ first, then determine $p(C_k|x)$ based on them.
        - advantage : marginal density of data $p(x)$ could be computed which then could be used to detect new data points that have low probability under the model and for which the predictions may be of low accuracy.
        - disadvantage : complexity is the highest. The class-conditional probability $p(x|C_k)$ contains a lot of irrelevant structural information.
    - **discriminative models** : determine $p(C_k|x)$ directly, then adopt the decision theory based on it.
    - **discriminatant function** : train a function which directly maps each input to its class.
        - disadvantage : without the posterior probability, a lot of issues rise to the surface.
- The significance of the posterior probability $p(C_k|x)$
    - *Minimizing risk* : when the loss matrix is constantly changing, with the posterior probability, the model could be modified trivially. However, without the posterior probability, we would have to train the model again from the beginning.
    - *Reject option* : the posterior probability allows us tp sift out error-leading input data.
    - *Compensating for class priors* : when the training set needs to be modified, for example, adjusting the ratio between different classes, we then have to compensate the effects of such modification by operating on the posterior probability.
    - *Combining models* : posterior probability allows us to combine the subproblems of the big problem together easily using the product rule.(The subproblems are independent from each other)
    $$ p(x_1, x_2 | C_k) = p(x_1|C_k)p(x_2|C_k)$$
    which is also called **naive Bayes model**
#### 1.5.5 Loss functions for regression
##### situation : determine a specific model $y(x)$ of each input x towards the target value t
- **Loss function** for the regression problem:
$$ E[L]=\int\int{L(t, y(x))p(x, t)dxdt}$$
- the goal is to choose a $y(x)$ to minimize $E[L]$
- when $L(t, y(x))$ is given by the form $|y(x)-t|^q$, the minimum is given by : $E_t(t|x)$, the conditional mean(**Minkowski loss**)
